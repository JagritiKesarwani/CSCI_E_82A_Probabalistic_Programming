{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Markov Networks\n",
    "\n",
    "\n",
    "\n",
    "## CSCI E-83\n",
    "## Stephen Elston\n",
    "\n",
    "in the previous lesson we explored directed Bayes networks (BNs). BNs are directed acyclic graphs (DAGs) which can represent causality with many probability distributions. Further, BNs can express independencies of probability distributions. However, there is no unique representation of independencies. Many BNs can exhibit the same independency structure, but with different causal relationships. \n",
    "\n",
    "Now, we will turn our attention to another class of probabilistic graphical models known as **Markov networks** (**MNs**) or **Markov graphical models**. Markov graphic models are not directed graphs. Yet, like BNs, Markov graphical models can represent probability distributions including the independencies. In fact, as you will see, there are methods to map from BNs to Markov networks and back. \n",
    "\n",
    "\n",
    "## Graph separation criteria\n",
    "\n",
    "You can develop a useful view of the relationship between BNs and MNs with the **D-separation** criteria, **directed-separation** criteria. First, we need to introduce a definition:  \n",
    "\n",
    "> **Definition:** An **immorality** in a directed graph $G$ occurs where either; a) there is a directed edge between $X$ and $Y$, or b) $X$ and $Y$ are both parents of the same note $Z$. \n",
    "\n",
    "This leads to a concept of a **moralized graph** that relates a directed BN to an undirected MN:\n",
    "\n",
    "> **Definition:** A **moral graph**, $M(G)$, of a BN structure, $G$ over $X$ is the **undirected graph** over $X$ that contains an undirected edge between $X$ and $Y$ if; a) there is a directed edge between $X$ and $Y$, or b) $X$ and $Y$ are both parents of the same note $Z$.\n",
    "\n",
    "This leads us to a corollary of relating the independencies of the directed BN to the independencies of a MN:\n",
    "\n",
    "> **Corollary:** Given a distribution $P_B$ such that $B$ is a parameterization on a graph $G$, then $M(G)$ is an I-map for $P_B$.\n",
    "\n",
    "What does all of this mean? In summary, we can create an MN with the same independencies as a BN. The example in the Figure below will help to illustrate this idea. \n",
    "\n",
    "<img src=\"img/MoralizedGraph.JPG\" alt=\"Drawing\" style=\"width:600px; height:200px\"/>\n",
    "<center> **Example of Graph Moralization** </center>\n",
    "\n",
    "Referring to the above illustration: \n",
    "1. The original DAG is shown on the left. The variable Z is the evidence variable. Z is said to **block** the path from X to W. \n",
    "2. The undirected **ancestral** graph is shown in the center. Since W has a blocked path in the DAG it is not part of the ancestral graph. \n",
    "3. We can make the following assertion about the DAG; $P(X,U\\ \\| Z) = P(X\\ \\| Z)\\ P( U\\ |\\ Z)$.  This situation is an **immorality** since X and U are parents of Z. In other words, X and U are independent given evidence Z, which **blocks** the path from X and U. The graph is **moralized** by adding an edge between X and U as shown on the right. \n",
    "\n",
    "Given the two definitions and the corollary, the resulting moralized undirected graph has the same independencies as the original DAG. If the DAG is an I-map of the distribution the undirected graph will also be an I-map. \n",
    "\n",
    "The conditional independencies in the moralied graph can be thought of as **separating** subsets of the Markov network. This leads to the following definition:\n",
    "\n",
    "> **Definition:** Given subsets $X$, $Y$ and $Z$, $X$ and $Y$ are conditionally independent or **D-separated** conditioned on the subset $Z$ if they are separated on the moralized graph. \n",
    "\n",
    "The definition leads to another related definition:\n",
    "\n",
    "> **Definition:** A graph $G$ is a **dependency map** or **D-map** of a distribution $P$ if the graph contains every conditional independence in $P$. We can represent this relationship as:\n",
    "\n",
    "$$(X\\ \\bot\\ Y\\ |\\ Z_G) \\Leftarrow (X\\ \\bot\\ Y\\ |\\ Z_P)$$\n",
    "\n",
    "Recalling the definition of an I-map we can create another definition.\n",
    "\n",
    "> **Definition:** If a graph $G$ is both an I-map and a D-map of a distribution $P$ we say that $G$ is a **prefect map** of $P$. We can write this relationships as:\n",
    "\n",
    "$$(X\\ \\bot\\ Y\\ |\\ Z_G) \\Leftrightarrow (X\\ \\bot\\ Y\\ |\\ Z_P)$$\n",
    "\n",
    "> **Note:** While it would be nice if a graph were a perfect map of a distribution, this will rarely be the case in real world problems. Thus, a perfect map is mostly useful as a reference point in developing probabilistic graphical models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation with MNs\n",
    "\n",
    "Markov networks can represent a probability distribution using **potentials** on an **undirected graph** $H$:\n",
    "\n",
    "> **Definition:** a distribution $P(X_1,\\ldots,X_n)$ can be represented by an undirected graphical graph $H$ using a set of positive **potential functions** $\\psi_c(X_c)$ associated with the cliques of $H$:\n",
    "\n",
    "$$P(X_1,\\ldots,X_n) = \\frac{1}{z} \\prod_{c \\in C} \\psi_c(X_c)\\\\\n",
    "\\text{where}\\\\\n",
    "Z = \\sum{X_1,\\ldots,X_n} \\prod_{c \\in C} \\psi_c(X_c)$$\n",
    "\n",
    "We call $Z$ the **partition function**. \n",
    "\n",
    "In other words, we can factor a probability distribution on a graphical model into potentials. Let's look at an example of factorizing a distribution on a graph, using the student letter example we worked on before.\n",
    "\n",
    "<img src=\"img/LetterDAG.JPG\" alt=\"Drawing\" style=\"width:600px; height:400px\"/>\n",
    "<center> **DAG for student letter and GRE score** </center>\n",
    "\n",
    "Referring to the above figure, we can factorize the directed graph or BH using conditional probabilities:\n",
    "\n",
    "$$P(I,D,G,S,L) = P(I)\\ P(D)\\ P(S\\ |\\ I)\\ P(G\\ |\\ I, D)\\ P(L\\ |\\ G)$$\n",
    "\n",
    "Notice that the **directed edges** of the graph provide **causal relationships**. \n",
    "\n",
    "With **undirected edges** we must model the **correlations** between the variables. We factorize the distribution on the MN using potentials. There is a potential for each **clique** on the **moralized** undirected graph. As a first step we need to create the undirected graph and moralize it:\n",
    "\n",
    "<img src=\"img/MoralizedLetter.JPG\" alt=\"Drawing\" style=\"width:600px; height:350px\"/>\n",
    "<center> **Transforming DAG to Moralized MN** </center>\n",
    "\n",
    "The graph is now undirected. The addition of the edge between $I$ and $D$ moralizes this graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship between conditional probability and potentials\n",
    "\n",
    "What is the relationship between a conditional distribution and the potentials in a graphical model? The relationship exists between ,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
    "\n",
    "$$p(x\\ \\| \\nu) = \\frac{1}{Z(\\nu)} \\prod_{s \\in \\mathcal{N}} \\psi_e(x_e)\\ \\prod_{(e,t) \\in }()$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do DAGs and Markov networks have equivalent representations?\n",
    "\n",
    "We have seen that we can transform a DAG into a Markov networks. The question should be, are these representations truly equivalent? In general, the answer is no. As we have seen, the Markov network contains additional links from the moralization of the directed graph. As a result, independence information can be lost. For example, consider a distribution $p(Z|X,Y)\\ p(X)\\ p(Y)$ represented by a Markov network for the with a single clique $\\phi(X,Y,Z)$. This representation does not allow us to infer the relationship $X\\ \\bot\\ Y$.   \n",
    "\n",
    "To address the question if we can create a DAG that can represent any Markov network, consider an example with the potentials $\\phi(W,X)\\ \\phi(X,Y)\\ \\phi(Y,Z)\\ \\phi(Z,W)$. There is no singly connected acyclic graph that can represent this relationship. Again, there is no equivalent representation.    \n",
    "\n",
    "From the above, you can see that there are limitations to the representations possible with either DAGs or Markov networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cliques on undirected graphs\n",
    "\n",
    "In order to factorize an undirected graph we must first decompose it into cliques. We can define a clique as follows:\n",
    "\n",
    "> **Definition:** A **clique** is a fully connected set of neighbors on an undirected graph. \n",
    "\n",
    "Let's apply this definition to the example in the figure below. \n",
    "\n",
    "<img src=\"img/Cliques1.JPG\" alt=\"Drawing\" style=\"width:600px; height:250px\"/>\n",
    "<center> **Cliques of a Chain Markov Network** </center>\n",
    "\n",
    "On the left side of the figure is a simple **chain** MN. There are only two possible **fully connected** cliques. These two cliques are $\\{1,2\\}$ and $\\{2,3\\}$ as illustrated above.\n",
    "\n",
    "The above example is simple, unrealistically so. The figure below shows a somewhat more complex undirected graph. \n",
    "\n",
    "<img src=\"img/Cliques2.JPG\" alt=\"Drawing\" style=\"width:600px; height:250px\"/>\n",
    "<center> **Cliques of a Undirected Graph** </center>\n",
    "\n",
    "The MN in the above diagram has two fully connected cliques; $\\{1,2,3\\}$ and $\\{2, 3, 4\\}$. The sets $\\{1,2\\}$, $\\{2,3\\}$, $\\{3,4\\}$, and $\\{1,4\\}$ cannot be cliques as each node has other connections that will form fully connected cliques. Further, $\\{1,2,4\\}$ and $\\{1, 3, 4\\}$ cannot be cliques as they are not fully connected. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorization on Markov networks\n",
    "\n",
    "Given this definition it is easy to find the cliques of our MN as illustrated in the figure below:\n",
    "\n",
    "<img src=\"img/LetterCliques.JPG\" alt=\"Drawing\" style=\"width:600px; height:400px\"/>\n",
    "<center> **Cliques of the Undirected Markov Network** </center>\n",
    "\n",
    "The undirected graph has three cliques; {D,I,G}, {I,S} and {G,L}. Now, we are in a position to factorize the MN into a set of clique potentials:\n",
    "\n",
    "$$P(I,D,G,S,L) = \\frac{1}{Z} \\exp\\{ E(D,I,G) + E(G,L) + E(I,S) \\} \\\\\n",
    "= \\frac{1}{Z} \\psi(D,I,G)\\ \\psi(G,L)\\ \\psi(I,S)$$\n",
    "\n",
    "Where    \n",
    "$E() = $ **expectation** function,  \n",
    "$\\psi() = $ **clique potential**, and\n",
    "$Z = $ **Partition function**:\n",
    "\n",
    "$$Z = \\sum_{I,D,G,S,L} \\psi(D,I,G)\\ \\psi(G,L)\\ \\psi(I,S)$$\n",
    "\n",
    "To summarize, the joint probability distribution is modeled as the product of potentials of the undirected graph or MN.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Bayesian network semantics  \n",
    "\n",
    "We have seen how distributions can be factorized on Bayesian networks. We can make the following statements:\n",
    "\n",
    "1. Factorization on BNs is based on conditional distributions.  \n",
    "2. Factorization on a BN implies conditional independencies.   \n",
    "\n",
    "These facts lead to the following definition:\n",
    "\n",
    "> **Definition:** Given a distribution $P$ and $G$ is a Bayesian network, $P$ factorizes as a set of CDPs specified as the nodes of $G$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov properties and separation in undirected graphs\n",
    "\n",
    " > **Definition:** For a graph $G$, **disjoint** subsets $A$ and $B$ are separated by subset $S$ if every path from $A$ to $B$ passes through $S$ then $S$ **separates** $A$ and $B$. Or, if $S = \\emptyset$, then no path exists from $A$ to $B$, and $A$ and $B$ are **separated**. \n",
    " \n",
    " > **Definition:** For a graph $G$ with disjoint sets $A$, $B$ and $S$, where $S$ separates $A$ and $B$, then $A\\ \\perp\\ B\\ \\| S$.\n",
    " \n",
    "This definition leads to the concepts of **soundness** and **completeness**. \n",
    "\n",
    "> **Theorem:** For any graph $G$ that factorizes a distribution $P$ then $I(G) \\subseteq I(P)$. This relationship is known as the **soundness** property.  \n",
    "\n",
    "> **Claim:** For any graph $G$, with subsets $X$, $Y$ and $Z$, that factorizes a distribution $P$, if  $(X \\bot Y\\ |\\ Z) \\subseteq I(P)$ the $\\mathrm{d-sep}_G(X;Y\\ |\\ Z)$. This relationship is known as the **completeness** property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov blanket\n",
    "\n",
    "> **Definition:** A subset of $X$, $U$, is a **Markov blanket** in a distribution $P$ if $X \\notin U$ and if, $U$, is a minimal set of nodes following the relationship: \n",
    "\n",
    "$$(X\\ \\bot\\ \\chi - \\{X \\} - U\\ |\\ U)\\ \\in\\ I(P)$$.\n",
    "\n",
    "The concept of a Markov blanket is illustrated in the figure below. \n",
    "\n",
    "<img src=\"img/MarkovBlanket.JPG\" alt=\"Drawing\" style=\"width:300px; height:300px\"/>\n",
    "<center> **Markov blanket of a node in a graph** </center>   \n",
    "\n",
    "In the above illustration the nodes, $\\{ 2, 6, 7 \\}$ are the Markov blanket of node $4$. You can see that the Markov blanket is the minimum subset of nodes which influence the node $4$. Conversely, node $4$ can only influence nodes in its Markov blanket.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2018, Stephen F Elston. All rights reserved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
